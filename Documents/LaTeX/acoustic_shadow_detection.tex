%%
%% Copyright 2007, 2008, 2009 Elsevier Ltd
%%
%% This file is based on part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.  Contact Elsevier for this file.
%%

%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references
%% SP 2008/03/01 
%%
%% $Id: elsarticle-template-harv.tex 4 2009-10-24 08:22:58Z rishi $
%%
%% This template is based on the 'elsarticle-template-harv.tex', but has been modified for specific use with submissions to the journal Ultrasound in Medicine and Biology, June 2010, KJH
%%

% Use this set of document class options for submission
%\documentclass[preprint, authoryear]{elsarticle}

\documentclass[authoryear,preprint,review,12pt]{elsarticle}

% Use this set of document class options to obtain an approximate 2 column view, note that this is primarily intended to allow authors to determine line breaks for long equations.  It is NOT meant to identically reproduce how the article would look in print.
%\documentclass[3p,twocolumn,authoryear,12pt]{elsarticle}


%% if you use PostScript figures in your article
%% use the graphics package for simple commands
%% \usepackage{graphics}
%% or use the graphicx package for more complicated commands
%% \usepackage{graphicx}
%% or use the epsfig package if you prefer to use the old commands
%% \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
 \usepackage{lineno}
 
 %% The multirow package adds the ability to do multirow and 
 %% multicolumn spanning in LaTeX.  This package is used 
 %% as an example for this template in the tables section.
 \usepackage{multirow}
 \usepackage{graphics}
 \usepackage{float}
 \usepackage{graphicx}% http://ctan.org/pkg/graphicx

\journal{Ultrasound in Medicine and Biology}

\begin{document}

\begin{frontmatter}

%% Title

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{Acoustic Shadow Detection: Study and Statistics of B-Mode and Radiofrequency Data}


%% Authors and addresses/affiliations

%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes; note the corresponding author can be any of the authors, but one author must be designated; here the third author has been arbitrarily designated as the corresponding author as an example.
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:

%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\author[Affil1]{Ricky Hu \corref{cor1}}
\author[Affil1]{Rohit Singla}
\author[Affil1]{Farah Deeba}
\author[Affil1,Affil2]{Robert N. Rohling}

\address[Affil1]{Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, Canada}
\address[Affil2]{Department of Mechanical Engineering, University of British Columbia, Vancouver, Canada}
% Replace capitalized text with the appropriate information (use standard capitalization rules for your text, not all capitals.
\cortext[cor1]{Corresponding Author: Ricky Hu, Robotics and Control Laboratory, University of British Columbia, Room 3090, 2332 Main Mall, Vancouver, BC, Canada V6T 1Z4.  Email: rhu@ece.ubc.ca}


\begin{abstract}
	An acoustic shadow is an ultrasound artifact occurring at boundaries between significantly different tissue impedances, resulting in signal loss and a dark appearance. Shadow detection is important as shadows can identify anatomical features or obscure regions of interest. A study was performed to scan human subjects (N=35) specifically to explore the statistical characteristics of various shadows from different anatomy and with different transducers. Differences in shadow statistics were observed and used for shadow detection algorithms with a fitted Nakagami distribution on radiofrequency speckle (RF) or cumulative entropy on brightness-mode (B-mode) data. The fitted Nakagami parameter and Entropy values in shadows were consistent across different transducers and anatomy. Both algorithms utilized adaptive thresholding, needing only the transducer pulse width as an input parameter for easy utilization by different operators or equipment. Mean Dice coefficients ($\pm$ standard deviation) of 0.90$\pm$0.07 and 0.87$\pm$0.08 were obtained for the RF and B-mode algorithms, which is within the range of manual annotators. The high accuracy in different imaging scenarios indicate that the shadows can be detected with high versatility and without expert configuration. The understanding of shadow statistics can be used for more specialized techniques to be developed for specific applications in the future, including pre-processing for machine learning and automatic interpretation. 
\end{abstract}


\begin{keyword}
	%% keywords here, in the form: keyword \sep keyword.  You may use no more than 10 keywords.
	Acoustic Shadow \sep Ultrasound \sep Speckle \sep Radiofrequency \sep Segmentation
\end{keyword}

\end{frontmatter}

%% Do not remove the page break here.
\pagebreak

\linenumbers

%% MAIN TEXT INSTRUCTIONS

%% For all sections, subsections, and subsubsections, use the '*' to remove numbering, as demonstrated below.

%% Commands for figures and tables should not be included in the main body of the submitted version of this file (e.g. the figure and tabular environments).  Figure captions should be listed in this file, as shown below.  Tables and Table captions should be listed as a separate section at the end of this file, as shown below.  Many authors may wish to include figures and tables within the main text of their document will preparing their manuscript.  This may be done, however please comment out any of the lines prior to submission.

%% Because the Elsevier editorial process does not allow for the figure and tabular environments in the submitted document, you will be unable to use autonumbering (i.e. \label and \ref) for figures and tables. 

%%  If long equations are used in the document, authors should use a two column format to make sure that the equations will break at approximately the right places.  To do this, replace the class option 'review' with the following two class options '3p' and 'twocolumn'.  Keep in mind that the column width produced in '3p' is slightly narrower than the final printer version.  After inserting the appropriate line breaks in your equation, change the '3p' option back to 'review'.

%% For citations, use the commands \citep and \citet

%% BEGIN MAIN TEXT

%%%%%%%%%%% INTRODUCTION
\section*{Introduction}
\label{intro}
Ultrasound devices have become increasingly affordable and portable, encouraging applications such as point-of-care ultrasound \citep{Bouhemad2011}, novice usage \citep{Sippel2011}, and analysis by machine learning \citep{Ghose2013}. However, ultrasound is susceptible to unique artifacts that increase the difficulty of interpretation and processing of images. One artifact is an acoustic shadow, which occurs when an ultrasound wave crosses a boundary of two materials with high impedance differences \citep{Kremkau1986}. The wave is almost completely reflected and depicted beyond the boundary is a continuous dark region and a loss of anatomical features. Shadows occur in air-tissue, tissue-bone, and tissue-lesion interfaces. Shadows can aid interpretation, such as identifying gall stones \citep{Good1979} or spinal levels \citep{Galiano2005}. However, shadows, such as from poor transducer contact, can lead to misinterpretation of anatomy, particularly by novice users and automated processing algorithms. Thus, the identification of shadows is an important preprocessing step in many applications.

Several methods have been used in literature to detect shadows and illustrative examples are discussed. Geometric techniques model the path of an ultrasound signal for an expected image along the scanline using a random walk \citep{Karamalis2012}. Pixels are then flagged as a shadow if it is below a heuristic confidence threshold of 0.25. However, geometric techniques require knowledge of ultrasound transducer properties to parameterize random walk weights, such as the focal length, radius of curvature, and thickness. The technique is therefore challenging to implement across different ultrasound equipment. This also reduces applicability for machine learning applications as accurate transducer parameter labels are required for each image.

Pixel intensity methods ignore the transducer properties and analyze only the graphical properties of an image \citep{Hellier2010}. Shadows have been detected on brain images by analyzing the entropy along a scanline to flag pixels of sudden low entropy as a potential shadow. These techniques achieved a comparable Dice similarity coefficient as geometric methods but require specific thresholding, window sizing, filtering, and image mask parameterization for different anatomy and transducers. The drawback is again the need for parameterization and tuning, which requires image processing expertise and prior knowledge of specific applications.

Machine learning methods have gained significant interest in medical imaging analysis. To our knowledge, no machine learning method has demonstrated the capability of general shadow detection from multiple types of anatomy. Deep learning methods have identified features in a specific image sets that contain shadows, such as neuroanatomical regions in cranial scan \citep{Milletari2017} or spinal levels in a posterior scan \citep{Hetherington2017}. Although machine learning has the potential of providing automated feature recognition in multiple applications, a large data set is required for an algorithm to recognize certain features. Ultrasound imaging is highly variable due to unique artifacts, operator techniques, and equipment. In addition, shadows are a common feature that occur in various imaging scenarios. Previous techniques focused on a single anatomical region and training data was from a consistent imaging scenario. However, it is difficult to construct a training data set with the generality required to recognize shadows in different scenarios usable for a variety of ultrasound applications.

There are two objectives to this paper. First, to address the need for understanding general characteristics of shadows, a study was conducted to scan multiple anatomy and transducers specifically to analyze the statistics of different types of shadows. Second, to address existing needs for versatile detection with minimal parameterization, previous methods were then extended utilizing statistical thresholding of radiofrequency (RF) or brightness-mode (B-mode) data to detect shadows from various imaging scenarios. 

%%%%%%%%%%% MATERIALS AND METHODS
\section*{Materials and Methods}
\label{MaM}
\subsection*{Data Collection}
Ultrasound RF and B-mode data were acquired by scanning 37 adult participants with informed written consent, approved by the University of British Columbia Research Ethics Board (Study ID: H18-01199). The scans included a forearm scan near the distal end of the pronator quadratus, an elbow scan near the cubital fossa, and a rib scan on the anterior surface of right ribs 11-12. Each scan was taken with both a curvilinear (Model C5-2/60, Ultrasonix Medical Corporation, Richmond, BC, Canada) and linear (Model L14-5/38, Ultrasonix Medical Corporation, Richmond, BC, Canada) transducer. Different transducer settings were used for each anatomical region and transducer, summarized in Table 1. Shadows were expected to occur due to superficial and deep bones and from an air gap created by the lateral edges of the transducer not being in flush contact with the skin. The experiment was designed to generate a dataset from various imaging scenarios to explore general shadow characteristics and to validate the versatility of the two simple shadow detection methods. 

\subsection*{Radiofrequency Speckle Analysis}      
To analyze shadows, windows of speckle were analyzed on the RF signal. Speckle occurs due to multiplicative scattering of acoustic waves in a material, resulting in a granular appearance on the image. The benefit of RF analysis is that B-mode image processing commonly attempts to remove speckle, but speckle contains information related to the acoustic interactions in tissue \citep{Burckhardt1978}. Speckle can then characterize different regions, such as a region of tissue or a region of signal loss in a shadow. In addition, B-mode image formation can be manipulated by an operator to visually enhance an image, such as adjusting time-gain compensation or dynamic range. Thus, the underlying speckle analysis can provide shadow detection usable across different machines and operators.

One of the first models for speckle is the one parameter Rayleigh distribution to model the probability density of a random walk \citep{Burckhardt1978}. The Rayleigh distribution is capable of modeling fully developed speckle, which does not occur in limited scattering \citep{Tuthill1988}. More generalized models have been applied such as the Rician, Homodyned-K, and Nakagami distributions to characterize speckle \citep{Destrempes2010}. The utility of speckle has been demonstrated in the literature to classify tumorigenicity of breast lesions \citep{Byra2016} or levels of liver fibrosis \citep{Ho2012} by categorizing image regions based on the speckle pattern. Shadow characterization presents a simpler problem as shadow and non-shadow regions contain significantly different speckle patterns. Thus, the Nakagami distribution expressed in Eq. 1 was chosen to model speckle. The Nakagami distribution provides greater generality than the Rayleigh distribution while being more computationally efficient than the Rician or Homodyned K distributions \citep{Destrempes2010}:

\begin{equation}
\Phi(x,\mu,\omega) = 2(\frac{\mu}{\omega})^{\mu}\frac{1}{\Gamma(\mu)}x^{(2\mu-1)}e^{\frac{-\mu}{\omega}x^{2}}
\end{equation}where $x$ is RF intensity, $\mu$ is the shape parameter or Nakagami "m" parameter, $\omega$ is a scale parameter and $\Gamma(\mu)$ is the gamma distribution.

To characterize shadows, the raw RF data was first processed by computing the echo envelope of each scanline with a Hilbert transform. This was performed on an averaged RF signal from three image frames. This creates a pre-scan converted image, visually similar to B-mode but without filtering to remove speckle. Next, the RF image was divided into overlapped windows with a width of a single RF data point and a length of three times the pulse width. This patch size was demonstrated in literature to be sufficiently large to capture multiple wavelengths and scattering events while being small enough to be useful in differentiating different regions on the millimeter scale \citep{Byra2016}. Next, each window was fit to a Nakagami distribution using a maximum likelihood estimate to compute a map of Nakagami parameters $\mu$ and $\omega$, as shown in Fig. 1.

Then, for each ultrasound image, Otsu’s method was applied to its Nakagami $\omega$ map to automatically compute a $\omega$ threshold for each individual image as we expect separate distributions for shadow and non-shadow regions. This was sufficient as the $\omega$ parameter is significantly different for shadow regions with abundant speckle and non-shadow regions with minimal speckle, Then, for each scanline, the axially deepest data point that is above the threshold is labeled as the shadow boundary and all data points below are labeled as a shadow.

The Nakagami shape parameter, ‘m’, was also investigated, though there was not sufficient delineation between parameter values in shadow and non-shadow regions for this parameter to be effective in thresholding. The distributions of the two paraemters are displayed for shadow and non-shadow regions in Figure [XX].
\subsection*{B-mode Scanline Analysis}

Many ultrasound machines do not provide access to RF data for speckle analysis. Thus, a previous pixel-intensity shadow detection method on B-mode images was modified and extended. Scanline entropy was investigated on B-mode images to characterize different types of shadows, but with the addition of adaptive thresholding of entropy to address the need for usability with minimum configuration. B-mode analysis was performed on  an averaged image from 3 image frames, similar to RF analysis. First, the cumulative scanline entropy is computed for each pixel, similar to the ``Rupture Criterion" \citep{Hellier2010}, with the window size fixed as three times the pulse width, $\eta$, as defined in Eq. 2. This is the same window size as the RF analysis.

\begin{equation}
S_{i,j} = \sum_{k=1}^{3\eta}I(i-k,j)log_{2}\frac{I(i-k,j)}{I(i+k,j)}+I(i+k,j)log_{2}\frac{I(i+k,j)}{I(i-k,j)}
\end{equation} where $S_{i,j}$ is the cumulative entropy at pixel $i$ on scanline $j$, $\eta$ is the pulse width, and $I(i)$ is the gray level, or intensity, of pixel $(i,j$. For the case of curvilinear images, radial scanlines were linearly interpolated between the two symmetric lateral edges of the image.

Next, Otsu's method is applied onto the entropy map of each image to automatically compute a threshold entropy value, similar to RF analysis. The intuition of the threshold is different than in RF analysis. In RF analysis, the threshold separates patches of intense and minimal speckle. In B-mode analysis, the threshold separates pixels of a shadow boundary, which has high entropy, and pixels away from shadow boundary, which include shadow and non-shadow regions. Thus, shadows can be identified by finding the last pixel on a scanline with an entropy higher than the threshold, representing a bright shadow boundary.


\subsection*{Validation}

A trained annotator (RH) manually outlined the boundary of the shadow regions on B-mode images. The manual regions were used as a gold standard, as manual identification is common in clinical practice and has been used in previous literature for comparison \citep{Hellier2010}. A Dice coefficient was computed to compare similarity of manual and automated shadow detection. The manual outline was used to define four regions for classification of statistical parameters: a non-shadow region above the boundary, a shadow region below the boundary, a ``transition region", which is a window defined as three pulse widths long axially below the boundary, and a ``deep shadow region", which is the data below the transition region. The validation was repeated with the RF and entropy window increased and decreased by 50\%. The Ljung-Box Q-test was used to measure residual autocorrelation of the Dice coefficients.
%
%%%%%%%%%%% Results
\section*{Results}
\label{Results}
Examples of detected shadows from both methods are highlighted in gray in Fig. 2 in different shadow detection scenarios. The Dice coefficients for both methods for different anatomy and transducers are shown in Table 2. The mean Dice coefficients ($\pm$ standard deviation) were 0.90$\pm$0.07 and 0.87$\pm$0.08 for RF and B-mode methods. Manual annotation was repeated five times with a mean Dice coefficient of 0.92$\pm$0.02 for all images and transducers. The Dice coefficient did not change by more than 0.03 when the window size was varied by 50\%.

With the benefit of a varied dataset, general statistics of shadows can be analyzed, as summarized in Table 3 and Table 4. For shadow detection, the parameters differentiating a shadow and non-shadow are of particular interest. Shadows were observed to have a mean Nakagami $\omega$ parameter of 4.14 $\pm$ 0.40 and a mean entropy of 1.03 $\pm$ 0.29 whereas non-shadows were observed to have a mean $\omega$ of 6.24 $\pm$ 0.92 and 2.20 $\pm$ 0.81. The values of entropy and Nakagami $\omega$ are consistent across different transducers and anatomical regions. The variance of entropy and Nakagami $\omega$ in one imaging region and transducer setting is less than the variance across different regions and transducers for shadows and non-shadows. 


%%%%%%%%%%% DISCUSSION
\section*{Discussion}
\label{Discuss}
The RF and B-mode shadow detection developed achieved a comparable Dice similarity coefficient to manual detection for all anatomy and transducer types (p $<$ 0.025). The previous studies using B-mode entropy reported a mean Dice coefficient of 0.91$\pm$0.07 between manual annotators  \citep{Hellier2010}. An important feature of shadow detection is being able to differentiate between a shadow and simply high attenuation of the signal. Both scenarios result in an eventual loss of signal. Shadow detection, however, has a characteristic high intensity shadow boundary before a significant loss in signal, compared to gradual signal losses in attenuation. The high Dice similarity coefficient indicates that both methods were capable of this distinction. This is also visualized in Fig. 2, where regions of low intensity without a bright shadow boundary were correctly labeled as non-shadow. The high accuracy supports the versatility of the detection method as both methods are able to identify shadows across different anatomy and transducers with minimum configuration. 

For a general observation for shadows, the computed Nakagami $\omega$ parameters of all manually outlined shadows indicate that there is a statistically significant difference between shadow and non-shadow regions, regardless of anatomy and transducer and even with the error in the transition regions considered. The speckle and its statistics from shadows is thus distinct from the speckle created by tissue, muscle, or fat. This observation can be utilized in the future for further analysis of shadows. 

In RF detection, both false positive and false negative errors most frequently occurred immediately below a shadow boundary as opposed to B-mode detection where errors were in various regions. To study the frequent areas of error further, the ``transition region" immediately below a manually annotated shadow boundary and a ``deep shadow region" below the transition region was investigated. The Nakagami $\omega$ parameter of transition regions  of all anatomy and transducers were within a standard deviation of both shadow and non-shadow regions. The deeper shadow regions were observed to have a lower Nakagami $\omega$ parameter than shadow regions and with a lower standard deviation as summarized in Table 3. The spread of the speckle also significantly decreases after the transition region. This indicates that the transition region cannot be fully distinguished from either a shadow or non-shadow and presents as it is statistically similar to the two. This is likely the cause of the errors, as the speckle distribution is much more consistent in the deep shadow regions compared to any other region. Physically, speckle interactions appear to gradually lessen after a brightest point on a scanline, possibly due to incomplete total reflection at a boundary. The boundary is thus is not an instantaneous division between non-shadow and shadow, rather, there is a transition region with statistics between a shadow and non-shadow before the speckle fully resembles a shadow.

In the transition region of B-mode images, the entropy values were similar but consistently higher than non-shadow values. This is expected as entropy is the highest when there is the greatest change in pixel intensity, which occurs at a shadow boundary, even with the a non-instantaneous non-shadow to shadow transition. However, the averaged entropy of all non-shadow regions have a greater spread than the Nakagami parameters, likely due to the differing operator settings used. Thus, B-mode detection may not be as consistent as RF detection.

As both RF and B-mode images search for a threshold for the start of a shadow, it is possible to misinterpret a beginning of a shadow as a reverberation artifact. Reverberation at a shadow boundary would cause a similar bright region followed by a dark region, which visually appears like a shadow boundary despite being an artifact in a shadow region. This is addressed by considering directionality when searching for the start of a shadow boundary such that the first shadow boundary when traversing down a scanline is interpreted as a beginning of a shadow and any further shadow boundaries are interpreted as reverberation artifacts. Figure 2 shows shadow detection with a reverberation artifact underneath a shadow caused by the radial joint.

There is a limitation with analysis using the Nakagami distribution in that the Nakagami distribution modelling scatterers change depending on transducer frequency. Previous literature observed that in the 36-58MHz frequency range, the Nakagami $m$ parameter decreased near the theoretical lower limit compared to a higher Nakagami $m$ parameter value at 10MHz signal \citep{Cloutier2004}. This was reported to be due to the spatial organization of the cells being "on the order of a fraction of the wavelength" and a Nakagami distribution cannot model the scatterers of red blood cells at this frequency. Due to this and from limitations of the equipment used in our study, we cannot conclude that shadow detection with Nakagami analysis will be accurate in higher frequencies beyond the values tested. Future studies are required to analysis the performace of shadow detection in higher frequencies. Diagnostic ultrasound commonly uses a frequency range of 2-15MHz \citep{Jensen2007} and the shadow detection method is expected to not be applicable in most use cases without issues from the high frequency behaviour of the Nakagami distribution.

There is a limitation for diagnostic usage of the proposed shadow method in cases where acoustic shadowing does not exhibit the characteristic bright boundary followed by a dark region. In cases where there is partial or incomplete shadowing, such as small calcifications in the placenta (Abramowicz et al. 2008). In these cases, there is a resemblance of a shadow, where the calcification is brighter and the region below is noticeably darker, but not with a brightness difference as extreme as shadowing from the ulna and the regions below retain speckle similar to tissue. Although calcifications are pathologically important to recognize, the proposed shadow detection method would likely be unable to detect the partial shadowing from these calcifications. The proposed method would be applicable only in cases of more complete shadowing, which would still be practical for significant gall and kidney stones, for instance.

In previous literature, shadows were defined qualitatively \citep{Kremkau1986} as a sudden loss of signal and brightness. The observed transition region in this study suggests that the qualitative definition of a shadow may be insufficient for accurate detection. One algorithm may detect the shadow starting immediately after the brightest location, or another may use a convention such as a full width at half maximum to define where the signal has sufficiently low intensity to resemble the start of a shadow. There is a decision point required for a clear definition for where a shadow begins to improve shadow detection accuracy, both from a signaling perspective for image processing and a visual perspective for manual inspection. 


The findings in this study result in several implications. First, the statistics of acoustic shadows have been investigated on a dataset with shadows occurring from multiple scenarios as opposed to specific cases where shadows are observed. This provided a more generalizable observation that shadows can be characterized by distinctive speckle distributions in different of anatomy and equipment and that there exists a transition region before the loss of speckle in a shadow. Second, the shadow detection methods demonstrated high accuracy, indicating that the same shadow detection method can be used with different transducer or imaging location. In future studies, the speckle statistics observed can be used to develop further models for anatomical features containing shadows. In machine learning algorithms, an initial network could be used with the shadow detection methods presented. Future studies would also have to take into consideration the most frequent source of error of shadow detection as the shadow boundary.

%%%%%%%%%%% Conclusions
\section*{Conclusions}
\label{Conclusions}
Acoustic shadows from different imaging scenarios were investigated. RF and B-mode methods were developed for acoustic shadow detection requiring only the transducer pulse width as the input parameter. When comparing to manual detection, the methods achieved a Dice similarity coefficient within range of manual observers. The work focused on applying shadow detection and statistical analysis to a varied dataset of three different anatomical locations and two different transducer to provide a representative understanding of general acoustic shadows. The statistics of acoustic shadow indicate that shadows contain a distinct speckle distribution compared to non-shadows and the speckle characteristics transition at the shadow boundary. The statistical findings of shadows can aid interpretation of ultrasound images in the future using speckle analysis. The versatility of the shadow detection method has the potential to improve the interpretation of ultrasound images with shadow artifacts or to serve as a pre-processing step for machine learning methods.

        
%%%%%%%%%%% ACKNOWLEDGEMENTS
\section*{Acknowledgements}
\label{Ack}
This work is supported by the National Sciences and Engineering Research Council of Canada (Grant Number: F09-05533). We acknowledge Victoria Lessoway for training and assistance in manually identifying acoustic shadows.


%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section*{}
%% \label{}


%%%%%%%%%%% REFERENCES
%% REFERENCE FORMATTING INSTRUCTIONS

%% All bibliography information should be included using a 'thebibliography' environment.  Most authors will find it easiest to create a .bbl file using the commands \bibliographystyle{} and \bibliography{} and then copy and paste the contents of the .bbl file into the .tex file below, but before the figure captions section.  Examples for using the \bibliographystyle and \bibliography commands are listed below.  

%% Do not remove the page break here.
\pagebreak

%% References with bibTeX database, use this to create a .bbl file
\bibliographystyle{UMB-elsarticle-harvbib}
\bibliography{bibliography}

%% References copied and pasted from the .bbl file.  Copy and paste over the following two lines.  When using a bibTeX database to create a .bbl file, comment out the following two lines.
%\begin{thebibliography}{00}
%\end{thebibliography}

%%%%%%%%%%% FIGURE CAPTIONS

%% Include only the figure captions here (not the figures).  Figures are uploaded separately in the online Elsevier Editorial Submission process.

%% Do not remove the page break here.
\pagebreak

\section*{Figure Captions}

\begin{description}
\item[Figure 1:]  A visualization of the B-mode and RF parameter maps. The b) Entropy Map was computed from processing of the a) original B-mode image and the d) Nakagami $\omega$ map was computed from the c) echo envelope. Note that the echo envelope contains noticeable speckle, which has been used to fit a Nakagami distribution to characterize shadow. The region at depth 2.50 cm and scanlines 32-40 is attenuation and not a shadow. This is an important distinction in shadow detection and both maps show the region as below a threshold to flag a shadow boundary.
\item[Figure 2:]  A comparison of the original B-mode images, the detected shadows manual detection, RF detection, and B-mode detection. Both detection methods perform similarly to manual detection. Both methods perform slightly less accurately on curvilinear images, likely due to the reduced resolution from interpolating the scanlines. Most errors of RF detection occur near the shadow boundary, likely due to the transitioning speckle from non-shadow to shadow.
\end{description}


%%%%%%%%%%% TABLES AND TABLE CAPTIONS

%% Both tables and table captions should be included below.  Captions should appear above the table, as shown below. If users want to use multirow.sty, array.sty, etc., to fine control/enhance the tables, they are welcome to load any package of their choice and the elsearticle.cls should work in combination with all loaded packages.  For problems with loaded packages please contact: elsarticle@river-valley.com (the developers of the elsarticle document class) or support@elsevier.com (Elsevier customer support).

%% Since the tabular format is often difficult to work with for complex tables, authors may also choose to create their tables with another program.  Each table and it's corresponding caption should then be saved as a pdf.  Each pdf should then be uploaded separately during the online submission process.  If doing so, all of the text below concerning tables and table captions should be commented out.

%% If no tables are part of the manuscript, comment out or delete this entire section.

%% Do not remove the page break here.
\pagebreak

\section*{Tables}

\begin{description}
\item[Table 1:]  Transducer properties for different imaging scenarios. \\

\begin{table}[H]
	\begin{center}
			\begin{tabular}{|l|l|l|l|l|}
				\hline
				\textbf{}                                                                                              & \textbf{Anatomy} & \textbf{Frequency} & \textbf{Depth} & \textbf{Gain} \\ \hline
				\multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Linear\\ Transducer\\ (L14-5/38)\end{tabular}}}     & Forearm          & 11.0MHz            & 5.0cm          & 50\%          \\ \cline{2-5} 
				& Elbow            & 11.0MHz            & 5.0cm          & 40\%          \\ \cline{2-5} 
				& Ribcage          & 5.0MHz             & 10.0cm         & 30\%          \\ \hline
				\multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Curvilinear\\ Transducer\\ (C5-2/60)\end{tabular}}} & Forearm          & 4.0MHz             & 5.0cm          & 50\%          \\ \cline{2-5} 
				& Elbow            & 4.0MHz             & 5.0cm          & 40\%          \\ \cline{2-5} 
				& Ribcage          & 3.3MHz             & 10.0cm         & 30\%          \\ \hline
		\end{tabular}
	\end{center}
\end{table} 
\end{description}

\begin{description}
\item[Table 2:]  Mean Dice coefficients for different imaging scenarios $\pm$ standard deviation. 

\begin{table}[H]
	\begin{center}\begin{tabular}{|l|l|l|l|}
			\hline
			\textbf{}                                                                                 &         & \textbf{RF} & \textbf{B-Mode} \\ \hline
			\multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}Linear\\ (L14-5/38)\end{tabular}} & Forearm & 0.91$\pm$0.05                   & 0.89$\pm$0.06                             \\ \cline{2-4} 
			& Elbow   & 0.94$\pm$0.06                   & 0.90$\pm$0.07                             \\ \cline{2-4} 
			& Ribcage & 0.87$\pm$0.09                   & 0.84$\pm$0.06                             \\ \hline
			\multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}Curvilinear\\ (C5-2/60)\end{tabular}}                                                  & Forearm & 0.89$\pm$0.05                   & 0.86$\pm$0.08                             \\ \cline{2-4} 
			& Elbow   & 0.93$\pm$0.04                   & 0.90$\pm$0.09                             \\ \cline{2-4} 
			& Ribcage & 0.83$\pm$0.08                   & 0.83$\pm$0.10                             \\ \hline
			\textbf{Mean}      & \textbf{All Anatomy} & \textbf{0.90$\pm$0.07}           & \textbf{0.87$\pm$0.08}               \\ \hline
		\end{tabular}
	\end{center}
\end{table} 
\end{description}

\begin{description}
\item[Table 3]: The mean Nakagami $\omega$ and Entropy values of different anatomy, transducer, and shadowing region $\pm$ standard deviation. Values are consistent among different transducers and anatomical regions. The variance of entropy and Nakagami $\omega$ in one imaging region and transducer setting is less than the variance across different regions and transducers for shadows and non-shadows. 
\begin{table}[H]
	\begin{center}
		\resizebox{\columnwidth}{!}{\begin{tabular}{|l|l|l|l|l|l|l|}
			\hline
			& \multicolumn{3}{l|}{\textbf{\begin{tabular}[c]{@{}l@{}}Linear\\ (L14-5/38)\end{tabular}}} & \multicolumn{3}{l|}{\textbf{\begin{tabular}[c]{@{}l@{}}Curvilinear\\ (C5-2/60)\end{tabular}}} \\ \hline
			& Forearm & Elbow & Ribcage & Forearm & Elbow & Ribcage \\ \hline
			\multicolumn{7}{|l|}{\textbf{Nakagami $\omega$ (Log Scale)}} \\ \hline
			Shadow & 4.15 $\pm$ 0.45 & 4.18 $\pm$ 0.45 & 4.04 $\pm$ 0.42 & 4.22 $\pm$ 0.32 & 4.19 $\pm$ 0.40 & 4.08 $\pm$ 0.37 \\ \hline
			Non-Shadow & 6.19 $\pm$ 0.96 & 6.49 $\pm$ 0.97 & 6.29 $\pm$ 0.95 & 6.54 $\pm$ 0.88 & 6.29 $\pm$ 1.04 & 5.64 $\pm$ 0.71 \\ \hline
			Transition & 4.94 $\pm$ 0.62 & 5.36 $\pm$ 0.62 & 4.96 $\pm$ 0.38 & 5.26 $\pm$ 1.02 & 5.37 $\pm$ 0.99 & 4.59 $\pm$ 0.92 \\ \hline
			Deep Shadow & 4.13 $\pm$ 0.43 & 4.16 $\pm$ 0.43 & 4.03 $\pm$ 0.41 & 3.93 $\pm$ 0.20 & 4.09 $\pm$ 0.30 & 4.03 $\pm$ 0.26 \\ \hline
			\multicolumn{7}{|l|}{\textbf{Entropy (Log Scale)}} \\ \hline
			Shadow & 0.92 $\pm$ 0.22 & 1.10 $\pm$ 0.36 & 1.04 $\pm$ 0.27 & 1.06 $\pm$ 0.28 & 0.96 $\pm$ 0.21 & 1.10 $\pm$ 0.37 \\ \hline
			Non-Shadow & 2.34 $\pm$ 0.96 & 2.34 $\pm$ 0.80 & 2.14 $\pm$ 0.82 & 1.67 $\pm$ 0.82 & 1.75 $\pm$ 1.14 & 1.88 $\pm$ 0.42 \\ \hline
			Transition & 2.45 $\pm$ 0.62 & 2.56 $\pm$ 0.53 & 2.15 $\pm$ 0.51 & 2.18 $\pm$ 1.21 & 1.93 $\pm$ 1.10 & 1.99 $\pm$ 1.10 \\ \hline
			Deep Shadow & 0.71 $\pm$ 0.43 & 0.89 $\pm$ 0.26 & 0.92 $\pm$ 0.40 & 0.98 $\pm$ 0.21 & 0.82 $\pm$ 0.19 & 1.04 $\pm$ 0.26 \\ \hline
		\end{tabular}}
	\end{center}
\end{table} 
\end{description}

\begin{description}
	\item[Table 4]: The mean Nakagami $\omega$ and Entropy values of all anatomy and transducers for different shadowing regions $\pm$ standard deviation.
	\begin{table}[H]
		\begin{center}
					\begin{tabular}{|l|l|l|}
						\hline
						& \textbf{\begin{tabular}[c]{@{}l@{}}Mean Nakagami $\omega$ \\ (Log Scale)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}Mean Entropy\\ (Log Scale)\end{tabular}} \\ \hline
						Shadow & 4.14 $\pm$ 0.40 & 1.03 $\pm$ 0.29 \\ \hline
						Non-Shadow & 6.24 $\pm$ 0.92 & 2.02 $\pm$ 0.81 \\ \hline
						Transition & 5.08 $\pm$ 0.77 & 2.21 $\pm$ 0.84 \\ \hline
						Deep Shadow & 4.06 $\pm$ 0.34 & 0.89 $\pm$ 0.27 \\ \hline
				\end{tabular}
	\end{center}
	\end{table}
\end{description}

%%%%%%%%%%% VIDEO CAPTIONS

%% If submitting video's as a supplement to the manuscript, include only the video captions here (not the figures).  Videos are uploaded separately in the online Elsevier Editorial Submission process.  If videos are not included, comment out this section.

%% Do not remove the page break here.
\pagebreak

%\section*{Video Captions}
%
%\begin{description}
%\item[Figure 1:]  TYPE THE CAPTION FOR VIDEO ONE HERE.
%\item[Figure 2:]  TYPE THE CAPTION FOR VIDEO TWO HERE.
%\item[Figure 3:]  TYPE THE CAPTION FOR VIDEO ONE HERE.  CONTINUE THIS LIST FOR ALL OTHER CAPTIONS
%\end{description}
%


\end{document}
